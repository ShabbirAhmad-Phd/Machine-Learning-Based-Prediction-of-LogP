import pandas as pd

# Define the dataset based on the LaTeX table
data = {
    "Drug": [
        "Almotriptan", "Belzutifan", "Carmustine", "Eletriptan", "Captopril",
        "Rizatriptan", "Sumatriptan", "Telmisartan", "Zolmitriptan",
        "Dabrafenib", "Metronidazole", "Theobromine"
    ],
    "TPSA": [
        68.6, 92.5, 63.3, 68.6, 44.6, 68.6, 78.7, 72.6, 68.6, 105.0, 81.0, 61.8
    ],
    "Disease": [
        "Migraine", "Brain", "Brain", "Migraine", "Kidney", "Migraine",
        "Migraine", "Kidney", "Migraine", "Brain", "Kidney", "Kidney"
    ],
    r"$\mathcal{R}_{1}$": [
        142, 112, 149, 110, 98, 118, 98, 508, 98, 389, 135, 144
    ],
    r"$\mathcal{M}_{1}$": [
        122, 98, 126, 94, 84, 102, 84, 400, 84, 310, 112, 118
    ],
    r"$\mathcal{R}e\mathcal{ZG}_{3}$": [
        730, 730, 924, 602, 518, 682, 518, 2986, 518, 2290, 802, 828
    ],
    r"$\mathcal{N}$": [
        55.06, 45.30, 53.53, 42.19, 38.72, 44.52, 38.72, 171.27,
        38.72, 133.45, 48.22, 50.78
    ],
    r"$\mathcal{IN}_{1}$": [
        23.08, 19.23, 23.17, 18.52, 17.23, 18.92, 17.23, 70.42,
        17.23, 55.69, 20.71, 21.21
    ],
    r"$\mathcal{IN}_{2}$": [
        26.48, 22.15, 23.61, 21.39, 20.15, 21.84, 20.15, 80.04,
        20.15, 62.17, 21.95, 23.44
    ],
    r"$\mathcal{GQ}$": [
        23.36, 19.87, 18.49, 18.92, 17.89, 19.37, 17.89, 65.51,
        17.89, 51.02, 17.80, 19.10
    ],
    r"$\mathcal{QG}$": [
        27.08, 24.13, 29.46, 23.10, 22.10, 23.58, 22.10, 85.66,
        22.10, 67.50, 25.44, 26.03
    ]
}

# Create DataFrame
df = pd.DataFrame(data)

# Save as CSV file
csv_path = "topological_indices_dataset.csv"
df.to_csv(csv_path, index=False)

#import ace_tools as tools; tools.display_dataframe_to_user(name="Topological Indices Dataset", dataframe=df)

csv_path





from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# Step 1: Drop non-numeric columns and separate features and target
X = df.drop(columns=["Drug", "Disease", "TPSA"])
y = df["TPSA"]

# Step 2: Apply mean imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Step 3: Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Step 4 & 5: Reconstruct the DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
normalized_df = pd.concat([X_scaled_df, y], axis=1)

# Save the normalized dataset
normalized_csv_path = "normalized_topological_indices_dataset.csv"
normalized_df.to_csv(normalized_csv_path, index=False)

#tools.display_dataframe_to_user(name="Normalized Topological Indices Dataset", dataframe=normalized_df)

normalized_csv_path




import matplotlib.pyplot as plt
import seaborn as sns

# Prepare raw imputed data for comparison (before scaling)
X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)

# Set up the figure
plt.figure(figsize=(14, 6))

# Violin plot for raw data
plt.subplot(1, 2, 1)
sns.violinplot(data=X_imputed_df)
plt.title("Raw Feature Distributions (Mean Imputed)")
plt.xticks(rotation=45)

# Violin plot for normalized data
plt.subplot(1, 2, 2)
sns.violinplot(data=X_scaled_df)
plt.title("Normalized Feature Distributions (Standard Scaled)")
plt.xticks(rotation=45)

# Save the figure
violin_plot_path = "violin_plots_comparison.png"
plt.tight_layout()
plt.savefig(violin_plot_path, dpi=300)
plt.close()

# Save imputed raw data (before normalization)
raw_imputed_csv_path = "raw_imputed_topological_indices_dataset.csv"
X_imputed_df["TPSA"] = y
X_imputed_df.to_csv(raw_imputed_csv_path, index=False)

#tools.display_dataframe_to_user(name="Raw Imputed Feature Dataset", dataframe=X_imputed_df)

violin_plot_path, raw_imputed_csv_path





from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

results = []

# Loop through each feature
for feature in X_scaled_df.columns:
    x = X_scaled_df[[feature]].values
    model = LinearRegression().fit(x, y)
    y_pred = model.predict(x)

    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))  # FIXED
    intercept = model.intercept_
    slope = model.coef_[0]

    results.append({
        "Feature": feature,
        "R2": round(r2, 3),
        "MAE": round(mae, 2),
        "RMSE": round(rmse, 2),
        "Intercept": round(intercept, 3),
        "Slope": round(slope, 3),
        "Equation": f"TPSA = {intercept:.3f} + {slope:.3f}·{feature}"
    })






# Re-import necessary tools in case environment was reset
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Reuse previously defined X_scaled_df and y
X = df.drop(columns=["Drug", "Disease", "TPSA"])
y = df["TPSA"]

# Impute and scale again to ensure availability
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
imputer = SimpleImputer(strategy='mean')
scaler = StandardScaler()

X_imputed = imputer.fit_transform(X)
X_scaled = scaler.fit_transform(X_imputed)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Fit the multiple linear regression model
mlr_model = LinearRegression()
mlr_model.fit(X_scaled_df, y)
y_pred_mlr = mlr_model.predict(X_scaled_df)

# Calculate evaluation metrics
r2_mlr = r2_score(y, y_pred_mlr)
mae_mlr = mean_absolute_error(y, y_pred_mlr)
rmse_mlr = np.sqrt(mean_squared_error(y, y_pred_mlr))
intercept_mlr = mlr_model.intercept_
coefs_mlr = mlr_model.coef_

# Prepare DataFrame of coefficients
mlr_coef_df = pd.DataFrame({
    "Topological Index": X_scaled_df.columns,
    "Coefficient": np.round(coefs_mlr, 3)
})

# Save coefficients CSV
mlr_coef_csv_path = "mlr_coefficients.csv"
mlr_coef_df.to_csv(mlr_coef_csv_path, index=False)

# Diagnostic plots
residuals = np.array(y - y_pred_mlr, dtype=np.float64)
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Residual plot
sns.scatterplot(x=y_pred_mlr, y=residuals, ax=axes[0])
axes[0].axhline(0, color='red', linestyle='--')
axes[0].set_title("Residual Plot")
axes[0].set_xlabel("Predicted TPSA")
axes[0].set_ylabel("Residuals")

# Histogram with KDE
sns.histplot(residuals, kde=True, ax=axes[1], bins=10)
axes[1].set_title("Residual Distribution")
axes[1].set_xlabel("Residuals")

plt.tight_layout()
diagnostic_plot_path = "n-mlr_combined_plots.png"
plt.savefig(diagnostic_plot_path, dpi=300)
plt.close()

# Prepare regression equation string
regression_equation = f"TPSA = {intercept_mlr:.3f}"
for i, feat in enumerate(X_scaled_df.columns):
    sign = "+" if coefs_mlr[i] >= 0 else "-"
    regression_equation += f" {sign} {abs(coefs_mlr[i]):.3f}·{feat}"

diagnostic_plot_path, mlr_coef_csv_path, regression_equation, r2_mlr, mae_mlr, rmse_mlr







from sklearn.linear_model import ElasticNetCV, ElasticNet
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

# Define alpha ranges
alpha_ranges = {
    "Low": np.linspace(0.01, 10, 100),
    "Mid": np.logspace(-2, 2, 100),
    "High": np.linspace(1000, 2000, 100)
}

# Storage for results
en_models = {}
en_metrics = []

# Train ElasticNetCV for each alpha range
for range_label, alpha_values in alpha_ranges.items():
    en_cv = ElasticNetCV(alphas=alpha_values, l1_ratio=0.5, cv=5, random_state=42)
    en_cv.fit(X_scaled_df, y)
    y_pred = en_cv.predict(X_scaled_df)
    residuals = y - y_pred

    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    intercept = round(en_cv.intercept_, 3)
    non_zero_count = np.sum(en_cv.coef_ != 0)

    en_models[range_label] = {
        "model": en_cv,
        "predictions": y_pred,
        "residuals": residuals,
        "coefficients": en_cv.coef_
    }

    en_metrics.append({
        "Alpha Range": f"ElasticNet {range_label} Alpha",
        "Best Alpha": round(en_cv.alpha_, 3),
        "R2": round(r2, 3),
        "MAE": round(mae, 3),
        "RMSE": round(rmse, 3),
        "Intercept": intercept,
        "Non-zero Features": int(non_zero_count)
    })

# Plot RMSE vs Alpha for all ranges
plt.figure(figsize=(10, 6))
for range_label, alpha_values in alpha_ranges.items():
    model = en_models[range_label]["model"]
    rmse_path = np.sqrt(np.mean(model.mse_path_, axis=1))
    plt.plot(model.alphas_, rmse_path, label=f"{range_label} Alpha")
    best_alpha_idx = np.argmin(rmse_path)
    plt.scatter(model.alphas_[best_alpha_idx], rmse_path[best_alpha_idx], label=f"{range_label} Best α", marker='o')

plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('RMSE')
plt.title('RMSE vs Alpha for ElasticNet')
plt.legend()
plt.tight_layout()
rmse_plot_path = "en_rmse_alpha_comparison.png"
plt.savefig(rmse_plot_path, dpi=300)
plt.close()

# Save performance summary
en_summary_df = pd.DataFrame(en_metrics)
en_summary_csv = "elasticnet_summary_table.csv"
en_summary_df.to_csv(en_summary_csv, index=False)

#import ace_tools as tools; tools.display_dataframe_to_user(name="ElasticNet Summary Table", dataframe=en_summary_df)

rmse_plot_path, en_summary_csv








from sklearn.linear_model import LassoCV
import matplotlib.pyplot as plt

# Define LASSO alpha ranges
lasso_alpha_ranges = {
    "Low": np.linspace(0.01, 10, 100),
    "Mid": np.logspace(-2, 2, 100),
    "High": np.linspace(1000, 2000, 100)
}

lasso_models = {}
lasso_metrics = []

# Train LassoCV models for each alpha range
for range_label, alpha_values in lasso_alpha_ranges.items():
    lasso_cv = LassoCV(alphas=alpha_values, cv=5, random_state=42, max_iter=10000)
    lasso_cv.fit(X_scaled_df, y)
    y_pred = lasso_cv.predict(X_scaled_df)
    residuals = y - y_pred

    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    intercept = round(lasso_cv.intercept_, 3)
    non_zero_count = np.sum(lasso_cv.coef_ != 0)

    lasso_models[range_label] = {
        "model": lasso_cv,
        "predictions": y_pred,
        "residuals": residuals,
        "coefficients": lasso_cv.coef_
    }

    lasso_metrics.append({
        "Alpha Range": f"Lasso {range_label} Alpha",
        "Best Alpha": round(lasso_cv.alpha_, 3),
        "R2": round(r2, 3),
        "MAE": round(mae, 3),
        "RMSE": round(rmse, 3),
        "Intercept": intercept,
        "Non-zero Features": int(non_zero_count)
    })

# Plot RMSE vs Alpha for all Lasso ranges
plt.figure(figsize=(10, 6))
for range_label, alpha_values in lasso_alpha_ranges.items():
    model = lasso_models[range_label]["model"]
    mse_mean = np.mean(model.mse_path_, axis=1)
    rmse_path = np.sqrt(mse_mean)
    plt.plot(model.alphas_, rmse_path, label=f"{range_label} Alpha")
    best_alpha_idx = np.argmin(rmse_path)
    plt.scatter(model.alphas_[best_alpha_idx], rmse_path[best_alpha_idx], label=f"{range_label} Best α", marker='o')

plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('RMSE')
plt.title('RMSE vs Alpha for Lasso Regression')
plt.legend()
plt.tight_layout()

lasso_rmse_plot_path = "lasso_rmse_alpha_comparison.png"
plt.savefig(lasso_rmse_plot_path, dpi=300)
plt.close()

# Save summary as CSV
lasso_summary_df = pd.DataFrame(lasso_metrics)
lasso_summary_csv = "lasso_summary_table.csv"
lasso_summary_df.to_csv(lasso_summary_csv, index=False)

#import ace_tools as tools; tools.display_dataframe_to_user(name="Lasso Summary Table", dataframe=lasso_summary_df)

lasso_rmse_plot_path, lasso_summary_csv






# Fix: Import RidgeCV now
from sklearn.linear_model import RidgeCV

# Re-run refined Ridge regression after import
refined_ridge_alphas = np.linspace(0.001, 1.0, 200)
refined_ridge_cv = RidgeCV(alphas=refined_ridge_alphas, cv=5, scoring='neg_mean_squared_error')
refined_ridge_cv.fit(X_scaled_df, y)
y_pred_refined_ridge = refined_ridge_cv.predict(X_scaled_df)

# Extract metrics
refined_ridge_r2 = r2_score(y, y_pred_refined_ridge)
refined_ridge_mae = mean_absolute_error(y, y_pred_refined_ridge)
refined_ridge_rmse = np.sqrt(mean_squared_error(y, y_pred_refined_ridge))
refined_ridge_intercept = round(refined_ridge_cv.intercept_, 3)
refined_ridge_nonzero = np.sum(refined_ridge_cv.coef_ != 0)
refined_ridge_alpha = round(refined_ridge_cv.alpha_, 3)

# Coefficient bar plot
plt.figure(figsize=(8, 5))
plt.bar(X_scaled_df.columns, refined_ridge_cv.coef_)
plt.xticks(rotation=45)
plt.ylabel("Coefficient Value")
plt.title(f"Ridge Coefficients (α = {refined_ridge_alpha})")
plt.tight_layout()

refined_ridge_coef_plot = "n-refined_ridge_coefficients.png"
plt.savefig(refined_ridge_coef_plot, dpi=300)
plt.close()

# Format LaTeX-style regression equation
latex_ridge_eq = [f"$\\text{{TPSA}} = {refined_ridge_intercept} \\\\"]
for i, col in enumerate(X_scaled_df.columns):
    coef = refined_ridge_cv.coef_[i]
    sign = "+" if coef >= 0 else "-"
    latex_ridge_eq.append(f"$\\quad {sign} {abs(coef):.3f} \\cdot {col}$ \\\\")
latex_ridge_eq = "\n".join(latex_ridge_eq)

refined_ridge_alpha, refined_ridge_r2, refined_ridge_mae, refined_ridge_rmse, refined_ridge_nonzero, refined_ridge_coef_plot, latex_ridge_eq










from xgboost import XGBRegressor
from sklearn.utils import resample

from sklearn.model_selection import train_test_split
# Redefine train/test split for consistent context
X = df.drop(columns=["Drug", "Disease", "TPSA"])
y = df["TPSA"]
# Re-import train_test_split from sklearn.model_selection
# Re-split the dataset using the correct import
X = df.drop(columns=["Drug", "Disease", "TPSA"])
y = df["TPSA"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape


# Fix the error: use mean_squared_error without 'squared' for RMSE manually
def compute_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# Retry last step that failed due to 'squared' argument
xgb_bootstrap = {"rmse": [], "mae": [], "r2": []}
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, verbosity=0)

for i in range(100):
    X_bs, y_bs = resample(X_train, y_train)
    xgb_model.fit(X_bs, y_bs)
    y_pred_xgb = xgb_model.predict(X_test)

    xgb_bootstrap["rmse"].append(compute_rmse(y_test, y_pred_xgb))
    xgb_bootstrap["mae"].append(mean_absolute_error(y_test, y_pred_xgb))
    xgb_bootstrap["r2"].append(r2_score(y_test, y_pred_xgb))

# Compute mean ± std for XGBoost
xgb_bootstrap_summary = {
    "Model": "XGBoost Regressor",
    "RMSE Mean": round(np.mean(xgb_bootstrap["rmse"]), 2),
    "RMSE Std": round(np.std(xgb_bootstrap["rmse"]), 2),
    "MAE Mean": round(np.mean(xgb_bootstrap["mae"]), 2),
    "MAE Std": round(np.std(xgb_bootstrap["mae"]), 2),
    "R2 Mean": round(np.mean(xgb_bootstrap["r2"]), 2),
    "R2 Std": round(np.std(xgb_bootstrap["r2"]), 2)
}

xgb_bootstrap_summary
























